# generate node (context-mode, uses short memory from state.short_context)
from typing import List, Tuple
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

from state_schema import AgentState
from llm_utils import get_together_llm, count_tokens
from settings import TOGETHER_MODEL


def _latest_question(state: AgentState) -> str:
    """‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å state (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô input ‡πÉ‡∏´‡πâ LLM)"""
    return getattr(state, "user_query_latest", None) or state.user_query or ""


def _build_chat_memory(
    state: AgentState,
    max_items: int = 6,
    token_budget: int = 800,
) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á memory ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏à‡∏≤‡∏Å state.short_context:
    - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ó‡∏¥‡∏£‡πå‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÜ (1‚Äì2 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà upstream ‡πÉ‡∏™‡πà‡∏°‡∏≤)
    - ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á/‡∏ã‡πâ‡∏≥/‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ã‡∏±‡∏ö‡∏™‡∏ï‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô noise
    - ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÇ‡∏ó‡πÄ‡∏Ñ‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏¥‡∏ô context ‡∏´‡∏•‡∏±‡∏Å
    """
    latest = (_latest_question(state) or "").strip()
    raw = getattr(state, "short_context", None)

    items: List[str] = []
    if isinstance(raw, list):
        seen = set()
        for s in raw[-max_items:]:
            t = str(s or "").strip()
            if not t:
                continue
            if t in seen:
                continue
            if latest and (t == latest or t in latest):
                continue
            items.append(t)
            seen.add(t)

    memo_so_far = ""
    for it in items:
        prospective = memo_so_far + ("\n‚Ä¢ " if memo_so_far else "‚Ä¢ ") + it
        try:
            if count_tokens(prospective) > token_budget:
                break
        except Exception:
            # ‡∏ñ‡πâ‡∏≤ tokenizer ‡∏•‡πâ‡∏° ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
            break
        memo_so_far = prospective
    return memo_so_far


def _collect_docs_from_hits(
    state: AgentState,
    max_chunks: int = 8,
    token_budget: int = 3200,
) -> Tuple[List[Document], List[str]]:
    """
    ‡∏£‡∏ß‡∏° hits ‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° score ‚Üí ‡∏ï‡∏±‡∏î‡∏ï‡∏≤‡∏°‡∏á‡∏ö‡πÇ‡∏ó‡πÄ‡∏Ñ‡∏ô
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Document ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ chain ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ chunk_id ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á)
    """
    retrieval = getattr(state, "retrieval_result", None)
    hits_by_category = getattr(retrieval, "hits_by_category", {}) if retrieval else {}

    entries = []
    for cat, hits in hits_by_category.items():
        for h in (hits or []):
            entries.append((cat, h))

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
    entries.sort(key=lambda x: float(getattr(x[1], "score", 0.0) or 0.0), reverse=True)

    docs: List[Document] = []
    used_chunk_ids: List[str] = []
    used_ids_set: set[str] = set()
    context_so_far = ""

    for cat, hit in entries:
        chunk_id = getattr(hit, "chunk_id", None)
        if not chunk_id or chunk_id in used_ids_set:
            continue

        snippet = " ".join(
            s.strip() for s in (getattr(hit, "snippets", []) or []) if s and s.strip()
        )
        if not snippet:
            continue

        chunk_text = f"({cat}) {snippet}".strip()
        prospective = f"{context_so_far}\n\n{chunk_text}" if context_so_far else chunk_text

        try:
            if count_tokens(prospective) > token_budget:
                break
        except Exception:
            # ‡∏ñ‡πâ‡∏≤ tokenizer ‡∏•‡πâ‡∏° ‡∏Å‡πá‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÇ‡∏≠‡πÄ‡∏Ñ ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÑ‡∏õ
            pass

        docs.append(
            Document(
                page_content=chunk_text,
                metadata={
                    "chunk_id": chunk_id,
                    "score": float(getattr(hit, "score", 0.0) or 0.0),
                    "category": cat,
                },
            )
        )
        used_chunk_ids.append(chunk_id)
        used_ids_set.add(chunk_id)
        context_so_far = prospective

        if len(used_chunk_ids) >= max_chunks:
            break

    return docs, used_chunk_ids


def generate_node(state: AgentState) -> AgentState:
    """
    ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å hybrid_search_node:
      - ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ state.retrieval_result.hits_by_category ‡πÅ‡∏•‡πâ‡∏ß
      - ‡∏™‡∏£‡πâ‡∏≤‡∏á chat_memory ‡∏à‡∏≤‡∏Å state.short_context
      - ‡∏¢‡∏±‡∏î context + memory ‡∏•‡∏á system prompt ‡∏û‡∏£‡πâ‡∏≠‡∏° persona/guardrails
      - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    """
    question = _latest_question(state).strip()
    if not getattr(state, "retrieval_result", None) or not state.retrieval_result.hits_by_category:
        state.response = "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞"
        return state

    # ‡∏£‡∏ß‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô context
    docs, used_chunk_ids = _collect_docs_from_hits(state, max_chunks=8, token_budget=3200)
    if not docs:
        state.response = "‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞"
        return state

    # memory/chat_history ‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô‡∏à‡∏≤‡∏Å short_context
    chat_history = _build_chat_memory(state, max_items=6, token_budget=800)

    # Persona + guardrails + memory + context
    prompt = ChatPromptTemplate.from_messages(
[
    (
        "system",
        "‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó/Persona: ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏´‡∏±‡∏ï‡∏ñ‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏ó‡∏±‡∏ô‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏û‡∏π‡∏î‡πÅ‡∏ö‡∏ö‡πÑ‡∏ó‡∏¢\n"
        "‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏á‡∏≤‡∏ô: ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏´‡∏•‡∏±‡∏á‡∏´‡∏±‡∏ï‡∏ñ‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏≠‡∏ô‡∏ü‡∏±‡∏ô/‡∏ú‡πà‡∏≤‡∏ü‡∏±‡∏ô‡∏Ñ‡∏∏‡∏î/‡∏≠‡∏∏‡∏î‡∏ü‡∏±‡∏ô/‡∏Ç‡∏π‡∏î‡∏´‡∏¥‡∏ô‡∏õ‡∏π‡∏ô/‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏ó‡∏≤‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏õ‡∏≤‡∏Å) "
        "‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Äò‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‚Äô ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó\n"
        "‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠: ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° "
        "(‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏±‡∏ï‡∏ñ‡∏Å‡∏≤‡∏£, ‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥, ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô, ‡∏¢‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ/‡πÅ‡∏û‡πâ‡∏¢‡∏≤, ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß/‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏£‡∏†‡πå/‡∏¢‡∏≤‡∏•‡∏∞‡∏•‡∏≤‡∏¢‡∏•‡∏¥‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏î) ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢\n"
        "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢: ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡πà‡∏á‡∏¢‡∏≤‡πÄ‡∏≠‡∏á ‡∏´‡∏≤‡∏Å‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏°‡∏µ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏≤ ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤, ‡∏Ç‡∏ô‡∏≤‡∏î, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà, ‡∏´‡∏ô‡πà‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô "
        "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ ‚Äò‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏≤‚Äô ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡∏ô‡∏ï‡πÅ‡∏û‡∏ó‡∏¢‡πå/‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£\n"
        "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢/‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠: ‡∏´‡∏≤‡∏Å‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏û‡∏ö‡∏ó‡∏±‡∏ô‡∏ï‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏´‡πâ‡∏≠‡∏á‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n"
        "‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î ‡∏Ç‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠ ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô "
        "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô 24 ‡∏ä‡∏°.‡πÅ‡∏£‡∏Å / ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 2‚Äì3 / 1 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå) ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç+‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n"
        "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà ‚Äò‡∏Ñ‡∏á‡∏ó‡∏µ‡πà/‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤‚Äô ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô\n"
        "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞):\n"
        "- ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏±‡πâ‡∏ô 1-2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚Äò‡∏™‡∏£‡∏∏‡∏õ‚Äô, ‚Äò‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô ‡πÜ‚Äô, ‚Äò‡πÇ‡∏î‡∏¢‡∏™‡∏£‡∏∏‡∏õ‚Äô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏î ‡πÜ)\n"
        "- ‚úÖ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥\n"
        "- ‚õîÔ∏è‡∏Ñ‡∏ß‡∏£‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á\n"
        "- ü™•‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏π‡πÅ‡∏•/‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô\n"
        "- ‚ö†Ô∏è‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó)\n"
        "‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡πà‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô ‚Äú‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏•‡πà‡∏≤‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞?‚Äù\n"
        "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏à‡∏≤‡∏Å‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå\n\n"
        "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏¢‡πà‡∏≠):\n{chat_history}\n\n"
        "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô):\n{context}"
    ),
    ("human", "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {input}"),
]
)



    text: str | None = None
    try:
        llm = get_together_llm(model=TOGETHER_MODEL, temperature=0.1)
        qa_chain = create_stuff_documents_chain(llm, prompt)
        out = qa_chain.invoke({"input": question, "context": docs, "chat_history": chat_history})
        if isinstance(out, str):
            text = out
        elif isinstance(out, dict):
            text = out.get("output_text") or out.get("text") or out.get("answer") or str(out)
        else:
            text = str(out)
    except Exception as e:
        state.errors.append(f"generate_llm_error: {e}")
        text = None

    if not text:
        bullets = [d.page_content for d in docs[:6] if d.page_content.strip()]
        text = "‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠:\n" + "\n".join(f"- {b}" for b in bullets)

    refs = ", ".join(used_chunk_ids[:5])
    if refs:
        text = f"{text}\n\n(‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: {refs})"

    state.response = text
    return state
